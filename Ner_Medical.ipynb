{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: ligne 1: pythonÂ : commande introuvable\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_med = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"archive/Corona2.json\",\"r\") as f:\n",
    "    data_med = json.load(f)\n",
    "#data_med[\"examples\"][0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =[]\n",
    "for data in data_med[\"examples\"]:\n",
    "    dict_tmp = {}\n",
    "    dict_tmp[\"text\"] = data[\"content\"]\n",
    "    dict_tmp[\"entity\"] = []\n",
    "    for annot in data[\"annotations\"]:\n",
    "        start = annot[\"start\"]\n",
    "        end = annot[\"end\"]\n",
    "        labels = annot[\"tag_name\"].upper()\n",
    "        dict_tmp[\"entity\"].append((start,end,labels))\n",
    "    train_data.append(dict_tmp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(360, 371, 'MEDICINE'),\n",
       " (383, 408, 'MEDICINE'),\n",
       " (104, 112, 'MEDICALCONDITION'),\n",
       " (679, 689, 'MEDICINE'),\n",
       " (6, 23, 'MEDICINE'),\n",
       " (25, 37, 'MEDICINE'),\n",
       " (461, 470, 'MEDICALCONDITION'),\n",
       " (577, 589, 'MEDICINE'),\n",
       " (853, 865, 'MEDICALCONDITION'),\n",
       " (188, 198, 'MEDICINE'),\n",
       " (754, 762, 'MEDICALCONDITION'),\n",
       " (870, 880, 'MEDICALCONDITION'),\n",
       " (823, 833, 'MEDICINE'),\n",
       " (852, 853, 'MEDICALCONDITION'),\n",
       " (461, 469, 'MEDICALCONDITION'),\n",
       " (535, 543, 'MEDICALCONDITION'),\n",
       " (692, 704, 'MEDICINE'),\n",
       " (563, 571, 'MEDICALCONDITION')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"entity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pepto-Bismol'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"text\"][25:37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp_med = spacy.blank(\"en\") \n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    }
   ],
   "source": [
    "for train_expl in train_data:\n",
    "    txt = train_expl[\"text\"]\n",
    "    labels = train_expl[\"entity\"]\n",
    "    doc = nlp_med.make_doc(txt)\n",
    "    ents = []\n",
    "    for start,end,label in labels:\n",
    "        span = doc.char_span(start,end,label=label)\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "           ents.append(span)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
